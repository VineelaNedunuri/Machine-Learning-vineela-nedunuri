{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>BMI_category_normal range</th>\n",
       "      <th>BMI_category_obese (class I)</th>\n",
       "      <th>BMI_category_obese (class II)</th>\n",
       "      <th>BMI_category_obese (class III)</th>\n",
       "      <th>BMI_category_overweight</th>\n",
       "      <th>BMI_category_underweight</th>\n",
       "      <th>BP_category_elevated</th>\n",
       "      <th>BP_category_hypertension stage I</th>\n",
       "      <th>BP_category_hypertension stage II</th>\n",
       "      <th>BP_category_normal</th>\n",
       "      <th>gender_Men</th>\n",
       "      <th>gender_Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   age  cholesterol  gluc  smoke  alco  active  cardio  \\\n",
       "0   0  50.0            1     1      0     0       1       0   \n",
       "1   1  55.0            3     1      0     0       1       1   \n",
       "2   2  52.0            3     1      0     0       0       1   \n",
       "3   3  48.0            1     1      0     0       1       1   \n",
       "4   4  48.0            1     1      0     0       0       0   \n",
       "\n",
       "   BMI_category_normal range  BMI_category_obese (class I)  \\\n",
       "0                          1                             0   \n",
       "1                          0                             1   \n",
       "2                          1                             0   \n",
       "3                          0                             0   \n",
       "4                          1                             0   \n",
       "\n",
       "   BMI_category_obese (class II)  BMI_category_obese (class III)  \\\n",
       "0                              0                               0   \n",
       "1                              0                               0   \n",
       "2                              0                               0   \n",
       "3                              0                               0   \n",
       "4                              0                               0   \n",
       "\n",
       "   BMI_category_overweight  BMI_category_underweight  BP_category_elevated  \\\n",
       "0                        0                         0                     0   \n",
       "1                        0                         0                     0   \n",
       "2                        0                         0                     0   \n",
       "3                        1                         0                     0   \n",
       "4                        0                         0                     0   \n",
       "\n",
       "   BP_category_hypertension stage I  BP_category_hypertension stage II  \\\n",
       "0                                 1                                  0   \n",
       "1                                 0                                  1   \n",
       "2                                 1                                  0   \n",
       "3                                 0                                  1   \n",
       "4                                 0                                  0   \n",
       "\n",
       "   BP_category_normal  gender_Men  gender_Women  \n",
       "0                   0           1             0  \n",
       "1                   0           0             1  \n",
       "2                   0           0             1  \n",
       "3                   0           1             0  \n",
       "4                   1           0             1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df1= pd.read_csv('../Labb/data/new_dataset1.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>BMI</th>\n",
       "      <th>gender_Men</th>\n",
       "      <th>gender_Women</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>52.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>48.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>48.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   age  ap_hi  ap_lo  cholesterol  gluc  smoke  alco  active  cardio  \\\n",
       "0   0  50.0    110     80            1     1      0     0       1       0   \n",
       "1   1  55.0    140     90            3     1      0     0       1       1   \n",
       "2   2  52.0    130     70            3     1      0     0       0       1   \n",
       "3   3  48.0    150    100            1     1      0     0       1       1   \n",
       "4   4  48.0    100     60            1     1      0     0       0       0   \n",
       "\n",
       "    BMI  gender_Men  gender_Women  \n",
       "0  22.0           1             0  \n",
       "1  34.9           0             1  \n",
       "2  23.5           0             1  \n",
       "3  28.7           1             0  \n",
       "4  23.0           0             1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=pd.read_csv('../Labb/data/new_dataset2.csv')\n",
    "df2.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - Välja modell\n",
    "Välj 3-5 maskininlärningsmodeller, gärna så olika som möjligt. För varje dataset som vi skapade i uppgift 2.3\n",
    "gör följande:\n",
    "\n",
    "- train|validation|test split\n",
    "- skala datasetet med feature standardization och normalization (de görs inte samtidigt, utan i olika omgångar)\n",
    "- definiera hyperparametrar (param_grids) att testa för varje modell\n",
    "- använda GridSearchCV() och välja lämplig evalueringsmetric\n",
    "- gör prediction på valideringsdata\n",
    "- beräkna och spara evaluation score för ditt valda metric\n",
    "- checka bästa parametrarna för respektive modell\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosen models are:\n",
    "- 1. Logistic Regression\n",
    "- 2. KNN \n",
    "- 3. Decision Tree\n",
    "- 4. Random Forest\n",
    "- 5. Gaussian Naive Bayes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Logistic Regression: \n",
    "###### https://en.wikipedia.org/wiki/Logistic_regression\n",
    "- Logistic regression is a statistical model used for binary classification problems, which means it predicts the probability event taking place based on a set of input features. \n",
    "- Based on data it predicts that a patient having a disease or not based on features such as age, gender, blood pressure,BMI etc.Logistic function has a relationship between the input features and the output variable.\n",
    "- It is a widely used algorithm in various fields such as medicine, social sciences and economics.\n",
    "\n",
    "### 2. K-Nearest Neighbors (KNN):\n",
    "###### https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\n",
    "- KNN  is a non-parametric supervised machine learning algorithm used for both classification and regression tasks.\n",
    "- In this data, KNN can be used for predicting whether a patient has cardiovascular disease or not based on their health metrics.\n",
    "- It identifs the k closest data points (neighbors) in the training set to the given input data point. The algorithm then assigns the output value of the input data point based on the majority class of the k-nearest neighbors.\n",
    "- It is a widely used algorithm in various fields such as medicine, facial recognition,finance,text mining and recommendation systems.\n",
    "\n",
    "### 3. Decision Tree:\n",
    "###### https://en.wikipedia.org/wiki/Decision_tree\n",
    "- A decision tree is a supervised machine learning algorithm which is easy to understand, as it creates a tree-like structure that can be interpreted and visualized.\n",
    "- It can handle both categorical and numerical data and can be used for both classification (i.e., predicting a binary outcome such as \"disease\" or \"no disease\") and regression (i.e., predicting a continuous outcome such as blood pressure or cholesterol levels).\n",
    "- It is a widely used algorithm in various fields such as engineering, fraud detection,credit risk analysis and medical diagnosis.\n",
    "\n",
    "### 4. Random Forest:\n",
    "###### https://en.wikipedia.org/wiki/Random_forest\n",
    "- Random forest is a machine learning algorithm used for classification, regression, and other tasks. \n",
    "- It is an ensemble method that creates multiple decision trees and combines their outputs to make a final prediction.\n",
    "- In this algorithm,each tree is trained on a random subset of the original dataset and a random subset of the features, which helps to reduce overfitting and increase the models accuracy. \n",
    "- It is a widely used algorithm in various fields such as medicine, finance,marketing,image and speech recognition.\n",
    "\n",
    "### 5. Gaussian Naive Bayes:\n",
    "###### https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "- Gaussian Naive Bayes is a probabilistic classification algorithm based on Bayes theorem, which describes the probability of a hypothesis based on prior knowledge and new evidence.\n",
    "-  It is a widely used algorithm in various fields such as text classification, spam filtering, image recognitionand medical diagnosis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For first data df1:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train|validation|test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns of first dataframe: (69971, 20)\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows and columns of first dataframe:',df1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train1.shape = (55976, 19)\n",
      "X_val1.shape = (11196, 19)\n",
      "X_test1.shape = (2799, 19)\n",
      "y_train1.shape = (55976,)\n",
      "y_val1.shape = (11196,)\n",
      "y_test1.shape = (2799,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from predict_model import split_data\n",
    "\n",
    "# For df1 data train |val|test split data\n",
    "X_train1, X_val1, X_test1, y_train1, y_val1, y_test1 = split_data(\n",
    "    df=df1, target_col=\"cardio\",test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"{X_train1.shape = }\\n{X_val1.shape = }\\n{X_test1.shape = }\\n{y_train1.shape = }\\n{y_val1.shape = }\\n{y_test1.shape = }\\n\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- skala datasetet med feature standardization och normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling is standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                 ('LR', LogisticRegression(random_state=42))]),\n",
       " 'K-Nearest Neighbor': Pipeline(steps=[('scaler', StandardScaler()), ('KNN', KNeighborsClassifier())]),\n",
       " 'Decision Tree': Pipeline(steps=[('scaling', StandardScaler()),\n",
       "                 ('DT', DecisionTreeClassifier(random_state=42))]),\n",
       " 'Random Forest': Pipeline(steps=[('scaling', StandardScaler()),\n",
       "                 ('RF', RandomForestClassifier(random_state=42))]),\n",
       " 'Gaussian Naive Bayes': Pipeline(steps=[('scaling', StandardScaler()), ('NB', GaussianNB())])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from predict_model import scale_features\n",
    "pipelines = scale_features(scale_type='standard')\n",
    "pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define hyperparameters for choosen models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Logistic Regression hyperparameters\n",
    "log_param_grid = [ {'LR__C': [0.01, 0.1, 1, 10, 100], 'LR__penalty': ['l2'], 'LR__solver': ['lbfgs','newton-cg','sag','saga'],'LR__max_iter': [1000, 5000, 10000]}]\n",
    "\n",
    "# KNN hyperparameters\n",
    "knn_param_grid = [{'KNN__n_neighbors': [3, 5, 7, 9, 11], 'KNN__weights': ['uniform', 'distance']}]\n",
    "\n",
    "# Decision Tree hyperparameters\n",
    "tree_param_grid = [{'DT__max_depth': [5, 10, 20], 'DT__min_samples_split': [2, 5, 10]}]\n",
    "\n",
    "\n",
    "# Random Forest hyperparameters\n",
    "forest_param_grid = [{'RF__n_estimators': [10, 50, 100, 200], 'RF__max_depth': [5, 10, 20], 'RF__min_samples_split': [2, 5, 10]}]\n",
    "\n",
    "\n",
    "# GaussianNB hyperparameters\n",
    "Gaussian_param_grid = [{'NB__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Logistic Regression': log_param_grid,\n",
    "    'K-Nearest Neighbor': knn_param_grid,\n",
    "    #'Support vector machines linear': svm_param_grid,\n",
    "    'Decision Tree': tree_param_grid,\n",
    "    'Random Forest': forest_param_grid,\n",
    "    'Gaussian Naive Bayes': Gaussian_param_grid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "\n",
      "Logistic Regression\n",
      "\n",
      "Accuracy Score: 70.41%\n",
      "F1 Score: 0.69\n",
      "Best parameters: {'LR__C': 1, 'LR__max_iter': 1000, 'LR__penalty': 'l2', 'LR__solver': 'lbfgs'}\n",
      "=============================================\n",
      "\n",
      "K-Nearest Neighbor\n",
      "\n",
      "Accuracy Score: 68.08%\n",
      "F1 Score: 0.67\n",
      "Best parameters: {'KNN__n_neighbors': 11, 'KNN__weights': 'uniform'}\n",
      "=============================================\n",
      "\n",
      "Decision Tree\n",
      "\n",
      "Accuracy Score: 69.36%\n",
      "F1 Score: 0.68\n",
      "Best parameters: {'DT__max_depth': 10, 'DT__min_samples_split': 2}\n",
      "=============================================\n",
      "\n",
      "Random Forest\n",
      "\n",
      "Accuracy Score: 70.50%\n",
      "F1 Score: 0.69\n",
      "Best parameters: {'RF__max_depth': 10, 'RF__min_samples_split': 10, 'RF__n_estimators': 50}\n",
      "=============================================\n",
      "\n",
      "Gaussian Naive Bayes\n",
      "\n",
      "Accuracy Score: 68.47%\n",
      "F1 Score: 0.67\n",
      "Best parameters: {'NB__var_smoothing': 1e-09}\n"
     ]
    }
   ],
   "source": [
    "from predict_model import grid_search\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    print('=============================================\\n')\n",
    "    print(f'{model_name:}\\n')\n",
    "    score_file='results/accuracy_scores.txt'\n",
    "    grid_search(pipeline, param_grid, X_train1, y_train1, X_val1, y_val1,'dataset1_standard',score_file)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling is minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Logistic Regression': Pipeline(steps=[('scaler', MinMaxScaler()),\n",
       "                 ('LR', LogisticRegression(random_state=42))]),\n",
       " 'K-Nearest Neighbor': Pipeline(steps=[('scaler', MinMaxScaler()), ('KNN', KNeighborsClassifier())]),\n",
       " 'Decision Tree': Pipeline(steps=[('scaling', MinMaxScaler()),\n",
       "                 ('DT', DecisionTreeClassifier(random_state=42))]),\n",
       " 'Random Forest': Pipeline(steps=[('scaling', MinMaxScaler()),\n",
       "                 ('RF', RandomForestClassifier(random_state=42))]),\n",
       " 'Gaussian Naive Bayes': Pipeline(steps=[('scaling', MinMaxScaler()), ('NB', GaussianNB())])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pipelines_min = scale_features(scale_type='minmax')\n",
    "pipelines_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================\n",
      "\n",
      "Logistic Regression\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, pipeline in pipelines_min.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    print('==============================================\\n')\n",
    "    print(f'{model_name:}\\n')\n",
    "    grid_search(pipeline, param_grid, X_train1, y_train1, X_val1, y_val1,'dataset1_minmax',score_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For first data df2:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- train|validation|test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows and columns of second dataframe:',df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For df1 data train |val|test split data\n",
    "X_train2, X_val2, X_test2, y_train2, y_val2, y_test2 = split_data(\n",
    "    df=df2, target_col=\"cardio\", test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{X_train2.shape = }\\n{X_val2.shape = }\\n{X_test2.shape = }\\n{y_train2.shape = }\\n{y_val2.shape = }\\n{y_test2.shape = }\\n\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling is standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standad scaling for dataset 2\n",
    "pipelines_stand = scale_features(scale_type='standard')\n",
    "pipelines_stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model_name, pipeline in pipelines_stand.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    print(' ===============================================\\n')\n",
    "    print(f'{model_name:}\\n')\n",
    "    grid_search(pipeline, param_grid, X_train2, y_train2, X_val2, y_val2,'dataset2_standard',score_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling is minmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min max scaling for dataset 2\n",
    "pipelines_max = scale_features(scale_type='minmax')\n",
    "pipelines_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model_name, pipeline in pipelines_max.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    print(' ==================================================\\n')\n",
    "    print(f'{model_name:}\\n')\n",
    "    grid_search(pipeline, param_grid, X_train2, y_train2, X_val2, y_val2,'dataset2_minax',score_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vilket dataset väljer du och vilken modell väljer du? Använd den modellen du valt och träna på all data förutom testdatan.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### https://www.analyticsvidhya.com/blog/2019/08/11-important-model-evaluation-error-metrics/\n",
    "- Accuracy is a common metric used to measure overall model performance but it can be misleading if the dataset is imbalanced which means one class is much more prevalent than the other. In such cases we can consider precision, recall, and F1 score can be more informative.\n",
    "- So here F1 score and accuracy are consider to measure the model performance.\n",
    "\n",
    "- Based on the results that are saved in accuracy scores  text file, it seems that the Random Forest classifier is the best model because it achieved the highest F1 score and accuracy on both datasets.\n",
    "\n",
    "- While in datasets, it seems that dataset2_standard performed better than dataset1_standard as it achieved higher F1 and accuracy scores across all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset2 with standard scale: Random forest classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import  accuracy_score,f1_score\n",
    "from predict_model import evaluate_classification\n",
    "# split total df2 into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df2.drop('cardio', axis=1), df2['cardio'], test_size=0.2, random_state=42)\n",
    "#X_train.shape, y_train.shape\n",
    "\n",
    "# from data 2 train val test split function splits:train:val:test -so we neglecting test data \n",
    "#X_train = pd.concat([X_train2, X_val2])\n",
    "#y_train= pd.concat([y_train2, y_val2])\n",
    "#X_test= X_test2\n",
    "#y_test= y_test2\n",
    "\n",
    "# pipline and hyperparameters for Random Forest\n",
    "random_pipeline = Pipeline([('scaler', StandardScaler()),('rf', RandomForestClassifier())])\n",
    "param_grid_rf={'rf__max_depth': [10] ,'rf__min_samples_split': [10], 'rf__n_estimators': [100]}\n",
    "\n",
    "grid_search= GridSearchCV(estimator=random_pipeline, param_grid=param_grid_rf, scoring=\"f1\", cv=3,error_score='raise')\n",
    "\n",
    "\n",
    "# to fit that object to training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# predictions on test data\n",
    "y_pred = grid_search.predict(X_test)\n",
    "   \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 Ensemble\n",
    "Använd VotingClassifier() på datasetet som du valt och lägg in de bästa parametrarna för respektive\n",
    "modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the voting classifier for 5 models with their best parameters \n",
    "vote_clf = VotingClassifier(\n",
    "    [\n",
    "        (\"LR\", LogisticRegression(C=100, max_iter=1000, penalty ='l2', solver='saga')),\n",
    "        (\"KNN\", KNeighborsClassifier(n_neighbors = 11, weights='distance')),\n",
    "        (\"DT\", DecisionTreeClassifier(max_depth=10,min_samples_split=2)),\n",
    "        (\"RF\", RandomForestClassifier(max_depth=10,min_samples_split=10,n_estimators=100)),\n",
    "        (\"NB\", GaussianNB(var_smoothing = 1e-09)),\n",
    "    ],\n",
    "    voting=\"hard\",\n",
    ")\n",
    "\n",
    "\n",
    "# Train the voting classifier\n",
    "vote_clf.fit(X_train2, y_train2)\n",
    "\n",
    "# Evaluate the accuracy of the voting classifier on the test set\n",
    "y_pred_clf = vote_clf.predict(X_test2)\n",
    "accuracy = accuracy_score(y_test2, y_pred_clf)\n",
    "print(f'Accuracy of the voting classifier: {accuracy*100:.3f}%')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 Evalueringar\n",
    "Gör confusion matrices och classification reports för 2.4 och 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predict_model import evaluate_classification\n",
    "# for dataset 2  with standard scale for Random forest\n",
    "evaluate_classification(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for voting classifier\n",
    "evaluate_classification(y_test2, y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 \"Deploy\" - spara modell\n",
    "Börja med att plocka ut 100 slumpmässigt valda rader från ditt dataset. Exportera dessa 100 samples i\n",
    "test_samples.csv. Därefter tar du den bästa modellen och träna på all data vi har förutom de 100\n",
    "datapunkterna du plockade ut. Spara därefter modellen i en .pkl-fil med hjälp av joblib.dump(). För\n",
    "modellen kan du behöva använda argumentet compress för att komprimera om filstorleken för stor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick 100 randomly selected rows from dataset 2\n",
    "random_samples= df2.sample(n=100, random_state=42)\n",
    "random_samples.to_csv('data/test_samples.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-learning-tetaV3aO",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
